# -*- coding: utf-8 -*-
"""fine-tuning-distibert-full.ipynb

Automatically generated by Colaboratory.

"""

import re
import numpy as np
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tqdm.notebook import tqdm_notebook

!pip install transformers

from transformers import DistilBertTokenizer, TFDistilBertModel

c_train = pd.read_csv('/kaggle/input/traindatasettweet/final_train_val.csv')

c_train

def format(lst):
    formatted_string = ' '.join(map(str, lst))
    return formatted_string

import ast

format(ast.literal_eval(c_train['labels'][2]))

c_train['labels'] = c_train['labels'].apply(lambda x: format(ast.literal_eval(x)))

c_train

dataset = c_train.iloc[:,1:3]

dataset

tweet=dataset['tweet']

dataset

dataset['labels'] = dataset['labels'].str.split()

dataset

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_tweets(text):
    input_ids = []
    attention_masks = []
    token_type_ids = []

    for tweet in text:
        encoded = tokenizer.encode_plus(
            tweet,
            add_special_tokens=True,
            max_length=256,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors='tf'
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
        token_type_ids.append(encoded['token_type_ids'])

    return {
        'input_ids': tf.concat(input_ids, axis=0),
        'attention_mask': tf.concat(attention_masks, axis=0),
        'token_type_ids': tf.concat(token_type_ids, axis=0)
    }

train_tokens = tokenize_tweets(dataset['tweet'])

distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

input_ids = tf.keras.Input(shape=(256,), dtype=tf.int32)
attention_mask = tf.keras.Input(shape=(256,), dtype=tf.int32)
token_type_ids = tf.keras.Input(shape=(256,), dtype=tf.int32)

mlb = MultiLabelBinarizer()
train_labels = mlb.fit_transform(dataset['labels'])

train_labels.shape

from tensorflow.keras import regularizers
from tensorflow.keras.layers import Dropout
bert_output = distilbert_model(input_ids, attention_mask=attention_mask)[0]
dropout_rate=0.5
weight_decay=0.001
dropout_layer = Dropout(rate=dropout_rate)(bert_output[:, 0, :])
output = tf.keras.layers.Dense(len(mlb.classes_), activation='sigmoid',kernel_regularizer=regularizers.l2(weight_decay))(dropout_layer)

tf.keras.mixed_precision.set_global_policy('mixed_float16')

model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=output)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy',metrics=['binary_accuracy'])

train_inputs = [train_tokens['input_ids'], train_tokens['attention_mask'], train_tokens['token_type_ids']]

from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint

es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=200)
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

history = model.fit(train_inputs, train_labels, batch_size=16, epochs=10,callbacks=[es, mc])

test_data= pd.read_csv('/kaggle/input/covid-tweet-test/preprocessed_test_val (1).csv')

test_tokens = tokenize_tweets(test_data['tweet'])

test_inputs_bert = [test_tokens['input_ids'], test_tokens['attention_mask'], test_tokens['token_type_ids']]

model.predict(test_inputs_bert)

data = model.predict(test_inputs_bert)

threshold = 0.5

binary_data = (data >= threshold).astype(int)

activated_classes = mlb.inverse_transform(binary_data)

mlb_classes = mlb.classes_

print(mlb_classes)

test_data['labels']=activated_classes

test_data['labels'] = test_data['labels'].apply(lambda x: ' '.join(map(str, x)))

test_data.head(20)

test_tweet= pd.read_csv('test.csv')

test_data.to_csv('distilbert_fulltrain.csv',index=False)
